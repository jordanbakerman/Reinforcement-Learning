{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q+ - Planning and Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](dynaqplus_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Seed\n",
    "np.random.seed(802)\n",
    "\n",
    "# Grid size\n",
    "grid_rows = 6\n",
    "grid_cols = 9\n",
    "\n",
    "# Starting maze position\n",
    "start = [5,3]\n",
    "\n",
    "# Terminal state\n",
    "goal = [0,8]\n",
    "\n",
    "# Obstacles in the maze\n",
    "walls = [ [3, 1], [3, 2], [3, 3], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8] ]\n",
    "\n",
    "# Action set 0=Up 1=Down 2=Left 3=Right\n",
    "actions = range(4)\n",
    "\n",
    "# Reward at Each Step\n",
    "reward = 0\n",
    "\n",
    "# Discount rate\n",
    "gamma = 0.95\n",
    "\n",
    "# Step Size\n",
    "alpha = 0.1\n",
    "\n",
    "# Exploration Rate\n",
    "epsilon = 0.1\n",
    "\n",
    "# Set number of episodes to run\n",
    "episodes = 1000\n",
    "\n",
    "# Number of steps before the environment changes\n",
    "steps_change = 500\n",
    "\n",
    "# Number of planning steps\n",
    "n_planning_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Action Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(state, q_table, epsilon):\n",
    "    \n",
    "    # Randomly select action with probability epsilon\n",
    "    if np.random.uniform() < epsilon:\n",
    "        g_action = np.random.choice(actions)\n",
    "        \n",
    "    # Select the best action for the given state with probability 1-epsilon\n",
    "    else:\n",
    "        q_vals = q_table[state[0], state[1], :]\n",
    "        q_vals_max = np.where(q_vals==q_vals.max())[0]\n",
    "        g_action = np.random.choice(q_vals_max)\n",
    "    \n",
    "    return g_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action, iters, steps_change):\n",
    "    \n",
    "    # Define the maze walls based on iteration number\n",
    "    if iters<steps_change:\n",
    "        maze_walls = walls\n",
    "    else:\n",
    "        maze_walls = walls[:-1]\n",
    "    \n",
    "    # Get location\n",
    "    rownum, colnum = state\n",
    "    \n",
    "    # 0=Up\n",
    "    if action==0:\n",
    "        rownum = max( rownum - 1, 0 )\n",
    "    \n",
    "    # 1=Down\n",
    "    elif action==1:\n",
    "        rownum = min( rownum + 1, grid_rows - 1 )\n",
    "    \n",
    "    # 2=Left\n",
    "    elif action==2:\n",
    "        colnum = max( colnum - 1, 0 )\n",
    "    \n",
    "    # 3=Right\n",
    "    else:\n",
    "        colnum = min( colnum + 1, grid_cols - 1 )\n",
    "    \n",
    "    # Correct for walls\n",
    "    if [rownum, colnum] in maze_walls:\n",
    "        rownum, colnum = state\n",
    "    \n",
    "    # Specify reward\n",
    "    if [rownum, colnum] == goal:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "    \n",
    "    return [rownum, colnum], reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_ql = np.zeros([grid_rows, grid_cols, len(actions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the steps to reach the goal for each episode\n",
    "steps_vec_ql = []\n",
    "\n",
    "# Q-Learning\n",
    "for i in range(episodes):\n",
    "    \n",
    "    # Count steps per episode\n",
    "    steps_count = 0\n",
    "    \n",
    "    # Always start in the same position\n",
    "    state = start\n",
    "    \n",
    "    # Loop until the goal is reached\n",
    "    while state!=goal:\n",
    "        \n",
    "        # Choose action based on epsilon-greedy\n",
    "        action = greedy(state, q_table_ql, epsilon)\n",
    "        \n",
    "        # Find next state given current state and action\n",
    "        new_state, reward = step(state, action, i, steps_change)\n",
    "        \n",
    "        # Update the q-table with Q-Learning\n",
    "        q_table_ql[state[0],state[1],action] = q_table_ql[state[0],state[1],action] + \\\n",
    "            alpha * ( reward + gamma * max(q_table_ql[new_state[0],new_state[1],]) - \\\n",
    "            q_table_ql[state[0],state[1],action] )\n",
    "            \n",
    "        # Set new state to current state\n",
    "        state = new_state\n",
    "        \n",
    "        # Update steps count\n",
    "        steps_count += 1\n",
    "    \n",
    "    # Save steps per episode\n",
    "    steps_vec_ql.append(steps_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Sample State, Action and Reward, Next State Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sars_pairs(model):\n",
    "    \n",
    "    # Sample random index \n",
    "    index_sample = np.random.choice(range(len(model)))\n",
    "    \n",
    "    # Find the pairs from the dictionary keys and values\n",
    "    state_action = list(model.keys())[index_sample]\n",
    "    newstate_reward = list(model.values())[index_sample]\n",
    "    \n",
    "    # Specify values to return\n",
    "    state = [state_action[0], state_action[1]]\n",
    "    action = state_action[2]\n",
    "    new_state = [newstate_reward[0], newstate_reward[1]]\n",
    "    reward = newstate_reward[2]\n",
    "    tau = newstate_reward[3]\n",
    "    \n",
    "    return state, action, new_state, reward, tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Table and Evironment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_dynaq = np.zeros([grid_rows, grid_cols, len(actions)])\n",
    "model_dynaq = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:02<00:00, 357.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store the steps to reach the goal for each episode\n",
    "steps_vec_dynaq = []\n",
    "\n",
    "# Number of steps taken to map environment\n",
    "time_steps = 0\n",
    "\n",
    "# Q-Learning\n",
    "for i in tqdm(range(episodes)):\n",
    "    \n",
    "    # Count steps per episode\n",
    "    steps_count = 0\n",
    "    \n",
    "    # Always start in the same position\n",
    "    state = start\n",
    "    \n",
    "    # Loop until the goal is reached\n",
    "    while state!=goal:\n",
    "        \n",
    "        # Choose action based on epsilon-greedy\n",
    "        action = greedy(state, q_table_dynaq, epsilon)\n",
    "        \n",
    "        # Find next state given current state and action\n",
    "        new_state, reward = step(state, action, i, steps_change)\n",
    "        \n",
    "        # Update the q-table with Q-Learning\n",
    "        q_table_dynaq[state[0],state[1],action] = q_table_dynaq[state[0],state[1],action] + \\\n",
    "            alpha * ( reward + gamma * max(q_table_dynaq[new_state[0],new_state[1],]) - \\\n",
    "            q_table_dynaq[state[0],state[1],action] )\n",
    "        \n",
    "        #####################################################################################\n",
    "        \n",
    "        # Create tuples to map environment\n",
    "        state_action_key = tuple([state[0], state[1], action])\n",
    "        newstate_reward_val = tuple([new_state[0], new_state[1], reward, 0])\n",
    "\n",
    "        # Add state, action, reward, new_state to model\n",
    "        if state_action_key not in model_dynaq.keys():\n",
    "            model_dynaq[state_action_key] = newstate_reward_val\n",
    "\n",
    "        #####################################################################################\n",
    "            \n",
    "        # Set new state to current state\n",
    "        state = new_state\n",
    "        \n",
    "        # Update steps count\n",
    "        steps_count += 1\n",
    "    \n",
    "    # Save steps per episode\n",
    "    steps_vec_dynaq.append(steps_count)\n",
    "    \n",
    "    # Update q_table with planning\n",
    "    for j in range(n_planning_steps):\n",
    "        \n",
    "        # Sample state_action and reward_nextstate pair\n",
    "        s_now, action, s_next, reward, tau = sample_sars_pairs(model_dynaq)\n",
    "        \n",
    "        # Update the q-table\n",
    "        q_table_dynaq[s_now[0],s_now[1],action] = q_table_dynaq[s_now[0],s_now[1],action] + \\\n",
    "            alpha * ( reward + gamma * max(q_table_dynaq[s_next[0],s_next[1],]) - \\\n",
    "            q_table_dynaq[s_now[0],s_now[1],action] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Table and Evironment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table_dynaq_plus = np.zeros([grid_rows, grid_cols, len(actions)])\n",
    "model_dynaq_plus = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna-Q+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:16<00:00, 60.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Store the steps to reach the goal for each episode\n",
    "steps_vec_dynaq_plus = []\n",
    "\n",
    "# Number of steps taken to map environment\n",
    "time_steps = 0\n",
    "\n",
    "# Small constant -> new reward = reward + kappa * sqrt(tau)\n",
    "kappa = 0.01\n",
    "\n",
    "# Q-Learning\n",
    "for i in tqdm(range(episodes)):\n",
    "    \n",
    "    # Count steps per episode\n",
    "    steps_count = 0\n",
    "    \n",
    "    # Always start in the same position\n",
    "    state = start\n",
    "    \n",
    "    # Loop until the goal is reached\n",
    "    while state!=goal:\n",
    "        \n",
    "        # Choose action based on epsilon-greedy\n",
    "        action = greedy(state, q_table_dynaq_plus, epsilon)\n",
    "        \n",
    "        # Find next state given current state and action\n",
    "        new_state, reward = step(state, action, i, steps_change)\n",
    "        \n",
    "        # Update the q-table with Q-Learning\n",
    "        q_table_dynaq_plus[state[0],state[1],action] = q_table_dynaq_plus[state[0],state[1],action] + \\\n",
    "            alpha * ( reward + gamma * max(q_table_dynaq_plus[new_state[0],new_state[1],]) - \\\n",
    "            q_table_dynaq_plus[state[0],state[1],action] )\n",
    "        \n",
    "        #####################################################################################\n",
    "        \n",
    "        # Create tuples to map environment\n",
    "        state_action_key = tuple([state[0], state[1], action])\n",
    "        newstate_reward_val = tuple([new_state[0], new_state[1], reward, time_steps])\n",
    "\n",
    "        # Add state, action, reward, new_state to model\n",
    "        model_dynaq_plus[state_action_key] = newstate_reward_val\n",
    "\n",
    "        #####################################################################################\n",
    "            \n",
    "        # Set new state to current state\n",
    "        state = new_state\n",
    "        \n",
    "        # Update steps count\n",
    "        steps_count += 1\n",
    "        \n",
    "    # Save steps per episode\n",
    "    steps_vec_dynaq_plus.append(steps_count)\n",
    "    \n",
    "    # Update q_table with planning\n",
    "    for j in range(n_planning_steps):\n",
    "        \n",
    "        # Time steps counter\n",
    "        time_steps += 1\n",
    "        \n",
    "        # Sample state_action and reward_nextstate pair\n",
    "        s_now, action, s_next, reward, tau = sample_sars_pairs(model_dynaq_plus)\n",
    "        reward = reward + kappa * np.sqrt(time_steps - tau)\n",
    "        \n",
    "        # Update the q-table\n",
    "        q_table_dynaq_plus[s_now[0],s_now[1],action] = q_table_dynaq_plus[s_now[0],s_now[1],action] + \\\n",
    "            alpha * ( reward + gamma * max(q_table_dynaq_plus[s_next[0],s_next[1],]) - \\\n",
    "            q_table_dynaq_plus[s_now[0],s_now[1],action] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(q_table):\n",
    "    optimal_policy = np.zeros([grid_rows,grid_cols], dtype=str)\n",
    "\n",
    "    for i in range(grid_rows):\n",
    "        for j in range(grid_cols):\n",
    "            direction = np.argmax(q_table[i,j,:])\n",
    "            if direction==0:\n",
    "                optimal_policy[i,j]=\"U\"\n",
    "            elif direction==1:\n",
    "                optimal_policy[i,j]=\"D\"\n",
    "            elif direction==2:\n",
    "                optimal_policy[i,j]=\"L\"\n",
    "            else:\n",
    "                optimal_policy[i,j]=\"R\"\n",
    "\n",
    "    optimal_policy[goal[0],goal[1]] = \"G\"\n",
    "    return optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning Optimal Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['U', 'U', 'R', 'R', 'R', 'R', 'R', 'R', 'G'],\n",
       "       ['D', 'R', 'R', 'D', 'L', 'R', 'R', 'U', 'L'],\n",
       "       ['R', 'R', 'R', 'R', 'R', 'R', 'U', 'L', 'U'],\n",
       "       ['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U'],\n",
       "       ['U', 'L', 'D', 'D', 'L', 'U', 'U', 'U', 'U'],\n",
       "       ['R', 'U', 'L', 'L', 'L', 'U', 'U', 'U', 'U']], dtype='<U1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Q Learning Optimal Policy\")\n",
    "optimal_policy(q_table_ql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyna-Q Optimal Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'G'],\n",
       "       ['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'L'],\n",
       "       ['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U'],\n",
       "       ['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U'],\n",
       "       ['U', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L'],\n",
       "       ['U', 'L', 'L', 'L', 'L', 'L', 'L', 'L', 'L']], dtype='<U1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dyna-Q Optimal Policy\")\n",
    "optimal_policy(q_table_dynaq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyna-Q+ Optimal Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['U', 'U', 'L', 'R', 'U', 'U', 'D', 'R', 'G'],\n",
       "       ['L', 'D', 'U', 'D', 'R', 'R', 'D', 'U', 'U'],\n",
       "       ['U', 'U', 'D', 'U', 'R', 'D', 'R', 'L', 'U'],\n",
       "       ['U', 'U', 'U', 'U', 'U', 'U', 'U', 'U', 'U'],\n",
       "       ['R', 'R', 'L', 'L', 'R', 'R', 'R', 'R', 'U'],\n",
       "       ['U', 'L', 'L', 'R', 'R', 'U', 'R', 'D', 'L']], dtype='<U1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dyna-Q+ Optimal Policy\")\n",
    "optimal_policy(q_table_dynaq_plus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
